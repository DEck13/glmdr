
\documentclass[11pt]{article}

\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}

\newcommand{\REVISED}{\begin{center}\LARGE REVISED DOWN TO HERE\end{center}}

%\VignetteEngine{knitr::knitr}

\begin{document}

\title{Polishing GLM Fits}

\author{Charles J. Geyer}

\maketitle

<<options-width,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
@

\section{Introduction}

We want reported MLE and Fisher information better than what R
function \texttt{glm} outputs with its default tolerance.
We do not believe its computations are sufficiently accurate
(coping well with inexactness of computer arithmetic) to set its
tolerance very close to zero.

We will do better on our own.
R function \verb@glmdr:::make.mlogl@ computes accurately minus
the log likelihood
and two derivatives avoiding overflow, underflow, catastrophic cancellation,
and other problems of inexact computer arithmetic.

\section{R}

<<libraries>>=
library(glmdr, lib.loc = "../package/glmdr.Rcheck")
@

\begin{itemize}
\item The version of R used to make this document is \Sexpr{getRversion()}.
\item The version of the \texttt{glmdr} package used to make this document is
    \Sexpr{packageVersion("glmdr")}.
\item The version of the \texttt{knitr} package used to make this document is
    \Sexpr{packageVersion("knitr")}.
\end{itemize}

\section{Example I} \label{sec:ex-i}

\subsection{Fit Using R Function \texttt{glm}}

This is the complete separation example of Agresti (see help for this dataset
for source).
<<example-i-glm>>=
data(complete)
gout <- glm(y ~ x, family = "binomial", data = complete,
    x = TRUE)
summary(gout)
@
% compares OK with 8931 handout infinity.pdf

\subsection{A Useful Function}

Define the inverse logit function with careful computation that
avoids overflow, underflow, catastrophic cancellation, and other
issues with inexact computer arithmetic.
<<invlogit>>=
invlogit <- function(theta)
    ifelse(theta < 0,
        exp(theta) / (1 + exp(theta)),
        1 / (1 + exp(- theta)))
@

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-i-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
offs <- model.offset(mf)

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "binomial")
mout <- mlogl(coefficients(gout))
@

Check that hessian is correct.
<<example-i-check-fish>>=
theta.glm <- predict(gout)
p.glm <- invlogit(theta.glm)
q.glm <- invlogit(- theta.glm)
# only good for Bernoulli (zero-or-one-valued) response
fish.glm <- t(modmat) %*% diag(p.glm * q.glm) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

Look at eigenvalues of Fisher information matrix at the MLE found by
R function \texttt{glm}.
<<example-i-fish-eigen>>=
eout <- eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)
eout$values
@

Compare to default tolerance for R function \texttt{all.equal}.
<<example-i-how-small>>=
foo <- args(getS3method("all.equal", "numeric"))
foo
all.equal.default.tolerance <- eval(as.list(foo)$tolerance)
all.equal.default.tolerance
@

According to R function \texttt{all.equal} with its default
tolerance we do not have both
eigenvalues equal to zero.
<<example-i-eigenvalues-zero>>=
eout$values < all.equal.default.tolerance
@
Of course, we could change the tolerance, but to what?
We need something that works for all problems not just this one.
You need to already know the answer to choose the tolerance to
get the correct answer.  So that's no help.

\subsection{Newton Step}

Find Newton step.
<<example-i-newt>>=
newt.step <- solve(mout$hessian, - mout$gradient)
newt.step
@

\subsection{Direction of Recession}

Could this possibly be a direction of recession (DOR)?

Map to saturated model canonical parameter space.
<<example-i-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
newt.step.sat
@

This is obviously a DOR for this toy problem.  In general, we need code
to check.  The check here only works for Bernoulli regression, not
general binomial regression.  We also do not deal with zero components
(possibly computed inexactly) of \texttt{newt.step.sat}.
<<example-i-dor-check>>=
all(ifelse(newt.step.sat < 0, resp == 0,
    ifelse(newt.step.sat > 0, resp == 1, TRUE)))
@

We will refine this check in examples below.

\subsection{Limiting Conditional Model}

For this example,
we have found a DOR and hence need to take the limit in this direction.
Since we are not using a computer algebra system, like Mathematica
or Maple, we cannot actually take the limit.  Let us just take a
step that is 100 times the Newton step.

<<example-i-beta-too>>=
beta.too <- coefficients(gout) + 100 * newt.step
@

\pagebreak[3]
And redo the Fisher information calculation.
<<example-i-fish-lcm>>=
mout <- mlogl(beta.too)
eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values
@

Now there is no question about the eigenvalues being both zero
modulo inexactness of computer arithmetic.

<<example-i-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <-
    max(eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values)
@

\subsection{Conclusion}

The limiting conditional model (LCM) is completely degenerate.
It has no parameters to estimate.  We are done with this example.

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}.
<<example-i-clean>>=
rm(list = setdiff(ls(),
    c("invlogit", "all.equal.default.tolerance")))
@

\section{Example II}

\subsection{Fit Using R Function \texttt{glm}}

This is the quasi-complete separation example of Agresti
(see help for this dataset for source).
<<example-ii-glm>>=
data(quasi)
gout <- glm(y ~ x, family = "binomial", data = quasi, x = TRUE)
summary(gout)
@
% compares OK with 8931 handout infinity.pdf

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-ii-glm-fish>>=
<<example-i-glm-fish>>
@

Check hessian is correct.
<<example-ii-check-fish>>=
<<example-i-check-fish>>
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-ii-fish-eigen>>=
<<example-i-fish-eigen>>
@

Now comparing to default tolerance for R function \texttt{all.equal}
<<example-ii-eigenvalues-zero>>=
<<example-i-eigenvalues-zero>>
@
\noindent
does give the correct number of eigenvalues that are zero
modulo inexactness of computer arithmetic (we know this toy problem
was constructed to have one null eigenvalue of Fisher information).

\subsection{Newton Step}

Find Newton step.
<<example-ii-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could this possibly be a direction of recession (DOR)?
We suspect that because its components are not exactly round numbers
that it is not exactly.

Map to saturated model canonical parameter space.
<<example-ii-newt-sat>>=
<<example-i-newt-sat>>
@

This is not exactly a DOR.
<<example-ii-dor-check>>=
<<example-i-dor-check>>
@

But it is approximately.
<<example-ii-newt-sat-zap>>=
newt.step.sat.zap <- zapsmall(newt.step.sat)
newt.step.sat.zap
all(ifelse(newt.step.sat.zap < 0, resp == 0,
    ifelse(newt.step.sat.zap > 0, resp == 1, TRUE)))
@

\subsection{Limiting Conditional Model}

For this example, since the Newton step is not \emph{exactly} a DOR
but is \emph{approximately} a DOR.  Hence we cannot take the limit
in this direction, but probably can go uphill on the log likelihood
a long way in this direction.

<<example-ii-possible-dor>>=
beta.try <- outer(2^(0:7), newt.step)
beta.try <- sweep(beta.try, 2, coefficients(gout), "+")
beta.try

# directional derivative along Newton path
apply(beta.try, 1, function(beta) sum(mlogl(beta)$gradient * newt.step))
@

Since the directional derivative stays negative along the Newton path
(as far as we went), we know we can take a step $2^7 = 128$ times the
Newton step and still go downhill on minus the log likelihood
(uphill on the log likelihood itself).

<<example-ii-beta-too>>=
beta.too <- beta.try[nrow(beta.try), ]
@

And redo the Fisher information calculation.
<<example-ii-fish-lcm>>=
mout <- mlogl(beta.too)
eout <- eigen(mout$hessian, symmetric = TRUE)
eout$values
@

Now it is more clear that there is one zero eigenvalue
modulo inexactness of computer arithmetic.

<<example-ii-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-ii-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
nulls

nulls.sat <- modmat %*% nulls
nulls.sat
nulls.sat == 0
@

Here we have actually calculated the linearity exactly,
but, in general, we would have to worry about inexactness of computer
arithmetic.  We also deal here only with the special case that there
is only one null eigenvector.  We will generalize this in other examples.

<<example-ii-linearity>>=
linearity <- as.vector(nulls.sat == 0)
linearity
@

Fit the LCM.
<<example-ii-glm-lcm>>=
gout.lcm <- glm(y ~ x, family = "binomial", data = quasi,
    subset = linearity)
summary(gout.lcm)
@
% compares OK with 8931 handout infinity.pdf

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-ii-glm-vcov>>=
vcov(gout.lcm)
@

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}.
<<example-ii-clean>>=
<<example-i-clean>>
@

\section{Example III}

\subsection{Fit Using R Function \texttt{glm}}

This is the complete separation example of Geyer
(see help for this dataset for source).
<<example-iii-glm>>=
data(quadratic)
gout <- glm(y ~ x + I(x^2), family = "binomial",
    data = quadratic, x = TRUE)
summary(gout)
@
% technical report 672, file phaseTR.pdf
% does not have this output

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-iii-glm-fish>>=
<<example-i-glm-fish>>
@

Check hessian is correct.
<<example-iii-check-fish>>=
<<example-i-check-fish>>=
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-iii-fish-eigen>>=
<<example-i-fish-eigen>>
@

Now comparing to default tolerance for R function \texttt{all.equal}
<<example-iii-eigenvalues-zero>>=
<<example-i-eigenvalues-zero>>
@
\noindent
does \emph{not} give the correct number of eigenvalues that are zero
modulo inexactness of computer arithmetic (we know this toy problem
was constructed to have completely degenerate LCM).

\subsection{Newton Step}

Find Newton step.
<<example-iii-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could this possibly be a direction of recession (DOR)?

Map to saturated model canonical parameter space.
<<example-iii-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
data.frame(quadratic, newt.step.sat)
@

This is obviously a DOR for this toy problem.
As in Section~\ref{sec:ex-i} above, we do a non-general check,
because that is good enough when the LCM is completely degenerate.
<<example-iii-dor-check>>=
<<example-i-dor-check>>
@

\subsection{Limiting Conditional Model}

Also following Section~\ref{sec:ex-i} above,
we take a step that is 100 times the Newton step.

<<example-iii-beta-too>>=
<<example-i-beta-too>>
@

And redo the Fisher information calculation.
<<example-iii-fish-lcm>>=
<<example-i-fish-lcm>>
@

<<example-iii-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)$values))
@

Now there is no question about the eigenvalues being all zero
modulo inexactness of computer arithmetic.

\subsection{Conclusion}

The limiting conditional model (LCM) is completely degenerate.
It has no parameters to estimate.  We are done with this example.

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}.
<<example-iii-clean>>=
<<example-i-clean>>
@

\section{Example IV}

\subsection{Fit Using R Function \texttt{glm}}

This is the categorical data example of Geyer
(see help for this dataset for source).
<<example-iv-glm>>=
data(catrec)
gout <- glm(y ~ (.)^3, family = "poisson", data = catrec, x = TRUE)
summary(gout)
@
% technical report 672, file phaseTR.pdf
% does not have this output

Note that, unlike the preceding examples, R function \texttt{glm}
gives no warning of solutions at infinity.  So this is a false negative
for those warnings.

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-iv-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
offs <- model.offset(mf)

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "poisson")
mout <- mlogl(coefficients(gout))
@

Check hessian is correct.
<<example-iv-check-fish>>=
theta.glm <- predict(gout)
fish.glm <- t(modmat) %*% diag(exp(theta.glm)) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

\pagebreak[3]
Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-iv-fish-eigen>>=
<<example-i-fish-eigen>>
@

\subsection{Newton Step}

Find Newton step.
<<example-iv-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

Could this possibly be a direction of recession (DOR)?
We suspect that because its components are not exactly round numbers
that it is not exactly.

Map to saturated model canonical parameter space.
<<example-iv-newt-sat>>=
<<example-i-newt-sat>>
@

This is not exactly a DOR.
<<example-iv-dor-check>>=
all(ifelse(newt.step.sat < 0, resp == 0,
    ifelse(newt.step.sat > 0, resp == Inf, TRUE)))
@

But it is approximately.
<<example-iv-newt-sat-zap>>=
newt.step.sat.zap <- zapsmall(newt.step.sat)
newt.step.sat.zap
all(ifelse(newt.step.sat.zap < 0, resp == 0,
    ifelse(newt.step.sat.zap > 0, resp == Inf, TRUE)))
@

\subsection{Limiting Conditional Model}

For this example, since the Newton step is not \emph{exactly} a DOR
but is \emph{approximately} a DOR.  Hence we cannot take the limit
in this direction, but probably can go uphill on the log likelihood
a long way in this direction.

<<example-iv-possible-dor>>=
beta.try <- outer(2^(0:7), newt.step)
beta.try <- sweep(beta.try, 2, coefficients(gout), "+")

# directional derivative along Newton path
path.grad <- apply(beta.try, 1,
    function(beta) sum(mlogl(beta)$gradient * newt.step))
path.grad
@

Here we see that we cannot take a huge step along the Newton path.
The last point at which we are still going downhill on minus the log likelihood
(uphill on the log likelihood itself) is a step
<<example-iv-beta-too>>=
kstep <- max(which(path.grad < 0))
kstep
beta.too <- beta.try[kstep, ]
@

This step is $2^\Sexpr{(0:7)[kstep]} = \Sexpr{2^(0:7)[kstep]}$ times
the Newton step.

And redo the Fisher information calculation.
<<example-iv-fish-lcm>>=
<<example-ii-fish-lcm>>
@

Now it is more clear that there is one zero eigenvalue
modulo inexactness of computer arithmetic.
We cannot really expect more accuracy than
<<example-iv-fish-lcm-eigen-accuracy>>=
max(eout$values) * .Machine$double.eps
@
\noindent
and it is also clear that the negative eigenvalue is entirely due
to inexactness of computer arithmetic because Fisher information matrices
are positive semi-definite and cannot have negative eigenvalues.

<<example-iv-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-iv-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
# nulls.sat
any(nulls.sat == 0)
any(abs(nulls.sat) < all.equal.default.tolerance)
@

Here we have \emph{not} calculated the linearity exactly,
so we have to worry about inexactness of computer
arithmetic.

<<example-iv-linearity>>=
linearity <- as.vector(abs(nulls.sat) < all.equal.default.tolerance)
catrec[! linearity, ]
@

This agrees with (part of) Table 2 in \citet{geyer-gdor}
and with Section~10.4 of the supplementary material for \citet{eck-geyer}.

Fit the LCM.
<<example-iv-glm-lcm>>=
gout.lcm <- glm(y ~ (.)^3, family = "poisson", data = catrec,
    subset = linearity)
summary(gout.lcm)
@

This agrees
with Section~10.5 of the supplementary material for \citet{eck-geyer}.

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-iv-glm-vcov>>=
v <- vcov(gout.lcm)
v <- v[! is.na(coefficients(gout.lcm)), ]
v <- v[ , ! is.na(coefficients(gout.lcm))]
range(eigen(v, symmetric = TRUE, only.values = TRUE)$values)
@

\subsection{Clean Up}

Clean R global environment, except keep function \texttt{invlogit}.
<<example-iv-clean>>=
<<example-i-clean>>
@

\section{Example V}

\subsection{Fit Using R Function \texttt{glm}}

This is the sports example of Geyer
(see help for this dataset for source).
<<example-v-glm>>=
data(sports)
gout <- glm(cbind(wins, losses) ~ 0 + ., family = "binomial",
    data = sports, x = TRUE)
summary(gout)
@

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-v-glm-fish>>=
# extract model matrix, response vector, and offset vector
modmat <- gout$x
mf <- model.frame(gout)
resp <- model.response(mf)
n <- rowSums(resp)
y <- resp[ , "wins"]
offs <- model.offset(mf)

# reduce dimension of coefficient vector and model matrix
inies <- ! is.na(coefficients(gout))
modmat <- modmat[ , inies, drop = FALSE]
beta <- coefficients(gout)[inies]

# calculate minus log likelihood and two derivatives
mlogl <- glmdr:::make.mlogl(modmat, resp, offs, "binomial")
mout <- mlogl(beta)
@

Check hessian is correct.
<<example-v-check-fish>>=
theta.glm <- predict(gout)
p.glm <- invlogit(theta.glm)
q.glm <- invlogit(- theta.glm)
fish.glm <- t(modmat) %*% diag(n * p.glm * q.glm) %*% modmat
all.equal(mout$hessian, fish.glm, check.attributes = FALSE)
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-v-fish-eigen>>=
<<example-i-fish-eigen>>
@

\subsection{Newton Step}

Find Newton step.
<<example-v-newt>>=
<<example-i-newt>>
@

\subsection{Direction of Recession}

This looks very close to the generalized direction of recession (GDOR)
given by \citet[Table~4]{geyer-gdor} if we allow for the fact that
the last coefficient (for the hogs) is being constrained to be zero.

Map to saturated model canonical parameter space.
<<example-v-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
team.names <- colnames(gout$x)
team.plus <- apply(gout$x, 1, function(x) team.names[x == 1])
team.minus <- apply(gout$x, 1, function(x) team.names[x == - 1])
data.frame(plus = team.plus, minus = team.minus,
    step = zapsmall(newt.step.sat))
newt.step.sat
@

This is not exactly a DOR.
<<example-v-dor-check>>=
all(ifelse(newt.step.sat < 0, y == 0,
    ifelse(newt.step.sat > 0, y == n, TRUE)))
@

But it is approximately.
<<example-v-newt-sat-zap>>=
newt.step.sat.zap <- zapsmall(newt.step.sat)
newt.step.sat.zap
all(ifelse(newt.step.sat.zap < 0, y == 0,
    ifelse(newt.step.sat.zap > 0, y == n, TRUE)))
@

\subsection{Limiting Conditional Model}

For this example, since the Newton step is not \emph{exactly} a DOR
but is \emph{approximately} a DOR.  Hence we cannot take the limit
in this direction, but probably can go uphill on the log likelihood
a long way in this direction.

<<example-v-possible-dor>>=
beta.try <- outer(2^(0:7), newt.step)
beta.try <- sweep(beta.try, 2, beta, "+")

# directional derivative along Newton path
path.grad <- apply(beta.try, 1,
    function(beta) sum(mlogl(beta)$gradient * newt.step))
path.grad
@

Here we see that we cannot take a huge step along the Newton path.
The last point at which we are still going downhill on minus the log likelihood
(uphill on the log likelihood itself) is a step
<<example-v-beta-too>>=
<<example-iv-beta-too>>
@

This step is $2^\Sexpr{(0:7)[kstep]} = \Sexpr{2^(0:7)[kstep]}$ times
the Newton step.

And redo the Fisher information calculation.
<<example-v-fish-lcm>>=
<<example-ii-fish-lcm>>
@

<<example-v-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-v-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
dim(nulls.sat)
linearity <- apply(abs(nulls.sat), 1,
    function(x) max(x) < all.equal.default.tolerance)
data.frame(plus = team.plus, minus = team.minus,
    linearity)
@

Here we have \emph{not} calculated the linearity exactly,
so we have to worry about inexactness of computer
arithmetic.
But this is the correct linearity for this problem.

Fit the LCM.
<<example-v-glm-lcm>>=
gout.lcm <- glm(cbind(wins, losses) ~ 0 + ., family = "binomial",
    data = sports, subset = linearity)
summary(gout.lcm)
@

This agrees with (part of) Table 4 in \citet{geyer-gdor}
and with Section~8.4 of the supplementary material for \citet{eck-geyer}.

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-v-glm-vcov>>=
<<example-iv-glm-vcov>>
@

\subsection{Clean Up}

Clean R global environment.
<<example-v-clean>>=
<<example-i-clean>>
@

\section{Example VI}

\subsection{Fit Using R Function \texttt{glm}}

This is the big categorical data example of Eck
(see help for this dataset for source).
<<example-vi-glm>>=
data(bigcategorical)
gout <- glm(y ~ (.)^4, family = "poisson", data = bigcategorical,
    x = TRUE)
# summary(gout) # too much printout
@

Note that, like Example~IV above, R function \texttt{glm}
gives no warning of solutions at infinity.  So this is a false negative
for those warnings.

\subsection{Fisher Information Matrix}

First we look at the Fisher information matrix for this parameter value.
<<example-vi-glm-fish>>=
<<example-iv-glm-fish>>
@

Check hessian is correct.
<<example-vi-check-fish>>=
<<example-iv-check-fish>>
@

Look at eigenvalues of Fisher information matrix the MLE found by
R function \texttt{glm}.
<<example-vi-fish-eigen-make>>=
eout <- eigen(mout$hessian, symmetric = TRUE, only.values = TRUE)
@

Figure~\ref{fig:eigen} is made by the following code
<<example-vi-fish-eigen-plot,eval=FALSE>>=
plot(eout$values, ylab = "eigenvalues", log = "y")
abline(h = all.equal.default.tolerance)
@
\begin{figure}
\begin{center}
<<example-vi-fish-eigen,echo=FALSE>>=
<<example-vi-fish-eigen-plot>>
@
\end{center}
\caption{Eigenvalues of Fisher information matrix for MLE produced by
R function \texttt{glm}.  Solid line is default tolerance of
R function \texttt{all.equal}.}
\label{fig:eigen}
\end{figure}
This figure shows that the default tolerance for R function \texttt{all.equal}
doesn't do the right thing for this example.  As we said before, one could
use a different tolerance, but what?  It order to choose the tolerance that
gives the right answer for a problem, you already need to know the right answer!

\subsection{Newton Step}

Find Newton step.
<<example-vi-newt>>=
newt.step <- solve(mout$hessian, - mout$gradient)
# newt.step # too big to show
@

\subsection{Direction of Recession}

Could this possibly be a direction of recession (DOR)?

Map to saturated model canonical parameter space.
<<example-vi-newt-sat>>=
newt.step.sat <- as.vector(modmat %*% newt.step)
# newt.step.sat # too big to show
@

This is not exactly a DOR.
<<example-vi-dor-check>>=
all(ifelse(newt.step.sat < 0, resp == 0,
    ifelse(newt.step.sat > 0, resp == Inf, TRUE)))
@

But it is approximately.
<<example-vi-newt-sat-zap>>=
newt.step.sat.zap <- zapsmall(newt.step.sat)
# newt.step.sat.zap # too big to show
all(ifelse(newt.step.sat.zap < 0, resp == 0,
    ifelse(newt.step.sat.zap > 0, resp == Inf, TRUE)))
@

\subsection{Limiting Conditional Model}

For this example, since the Newton step is not \emph{exactly} a DOR
but is \emph{approximately} a DOR.  Hence we cannot take the limit
in this direction, but probably can go uphill on the log likelihood
a long way in this direction.

<<example-vi-possible-dor>>=
<<example-iv-possible-dor>>
@

Wow!  Theory says that the restriction of a strictly convex function to a line
is another strictly convex function, which must have a strictly decreasing
derivative.  But inexact computer arithmetic fails to preserve this theoretical
property, even when done as carefully as possible (without using multiple
precision arithmetic).

To be conservative, let us take the last step before the derivative
turns (temporarily) positive, which is probably just rounding error.
<<example-vi-beta-too>>=
kstep <- min(which(path.grad > 0)) - 1
kstep
beta.too <- beta.try[kstep, ]
@

And redo the Fisher information calculation.
<<example-vi-fish-lcm>>=
mout <- mlogl(beta.too)
eout <- eigen(mout$hessian, symmetric = TRUE)
sum(eout$values < all.equal.default.tolerance)
@

<<example-vi-eigenzero,include=FALSE,echo=FALSE>>=
.polish.max.eigenzero <- c(.polish.max.eigenzero,
    max(eout$values[eout$values < all.equal.default.tolerance]))
@

There are too many eigenvalues to show, but at least we have the
right dimension of the null space of the Fisher information matrix
for the LCM, according to Section~11.4 of the supplementary material
for \citet{eck-geyer}.

We now determine the linearity vector for the LCM following
\citet[Section~6.2.1]{eck-geyer}.
<<example-vi-nulls>>=
is.zero <- eout$values < all.equal.default.tolerance
# is.zero

nulls <- eout$vectors[ , is.zero, drop = FALSE]
# nulls

nulls.sat <- modmat %*% nulls
dim(nulls.sat)
linearity <- apply(abs(nulls.sat), 1, max) < all.equal.default.tolerance
sum(! linearity)
subset(bigcategorical, ! linearity)
@
This is the correct linearity, agreeing with Sections~11.5 and~11.7
of the supplementary material for \citet{eck-geyer}.

Fit the LCM.
<<example-vi-glm-lcm>>=
gout.lcm <- glm(y ~ (.)^4, family = "poisson",
    data = bigcategorical, subset = linearity)
# summary(gout.lcm) # too big to show
@

We can tell by looking at inverse Fisher information for this model
that it does not have solution at infinity.  So this is the correct
LCM.  We are done with this example.
<<example-vi-glm-vcov>>=
<<example-iv-glm-vcov>>
@

\section{Discussion}

So the trick of ``polishing'' the result of R function \texttt{glm}
is to do one (or perhaps more, but we only needed one for all of our
examples) Newton steps safeguarded by a line search.
\begin{itemize}
\item If the Newton direction is actually a DOR, then we can follow
    the line search all the way to infinity.
\item If the Newton direction is not exactly a DOR, but DOR exist,
    (so the MLE does not exist in the conventional sense),
    then we can follow the line search a long way but not all the way
    to infinity (our line search step will be many times what the
    Newton step would have been.
\item If no DOR exist (so the MLE does exist in the conventional sense
    and the result of R function \texttt{glm} is close to correct),
    then nothing bad happens. We just do very close to the Newton
    step and slightly improve the accuracy of the MLE.
\end{itemize}

After this operation, it is much easier to discover null eigenvalues
of the Fisher information matrix.  In all the examples the largest
calculated eigenvalue that we decided was truly zero was
\Sexpr{max(.polish.max.eigenzero)} which occured
in Example~\Sexpr{library(utils); as.roman(which(.polish.max.eigenzero == max(.polish.max.eigenzero)))} (not shown).

This is quite a bit below the cutoff we used
<<discuss-cutoff>>=
all.equal.default.tolerance
@

Presumably we could use
<<discuss-cutoff-too>>=
(.Machine$double.eps)^(3 / 4)
@
\noindent
or something of the sort.

We can special case the case of no eigenvalues zero (the MLE exists in
the conventional sense) and all eigenvalues zero
(the MLE in the Barndorff-Nielsen completion is completely degenerate).
In either case the LCM is trivial and need not be fitted.
In the first case, R function \texttt{glm} has already done the work.
In the second case, there is no work to do, because a completely degenerate
model has no identifiable parameters.

\begin{thebibliography}{}

\bibitem[Eck and Geyer(submitted)]{eck-geyer}
Eck, D.~J. and Geyer, C.~J. (submitted).
\newblock Computationally efficient likelihood inference
    in exponential families when the maximum likelihood estimator
    does not exist.
\newblock \url{https://arxiv.org/abs/1803.11240}.

\bibitem[Geyer(2009)]{geyer-gdor}
Geyer, Charles J. (2009).
\newblock Likelihood inference in exponential families and directions
    of recession.
\newblock \emph{Electronic Journal of Statistics}, \textbf{3}, 259--289.

\end{thebibliography}


\end{document}

