
R version 3.5.1 (2018-07-02) -- "Feather Spray"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
>  library(glmdr)
>  library(numDeriv)
> 
>  data(sports)
> 
>  gout <- glm(cbind(wins, losses) ~ 0 + ., family = "binomial",
+      data = sports, x = TRUE)
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
>  # summary(gout) # do not print summaries in tests, not same on all computers
> 
>  # extract model matrix, response vector, and offset vector
> 
>  modmat <- gout$x
>  mf <- model.frame(gout)
>  resp <- model.response(mf)
>  offs <- model.offset(mf)
> 
>  # modmat # DEBUG only
>  # resp   # DEBUG only
>  # offs   # DEBUG only
> 
>  # have to deal with dropped predictors
>  outies <- is.na(coefficients(gout))
> 
>  mymodmat <- modmat[ , ! outies]
> 
>  mlogl <- glmdr:::make.mlogl(mymodmat, resp, offs, "binomial")
> 
>  beta.test <- coefficients(gout)[! outies]
>  # we don't want to test where the gradient is nearly zero
>  beta.test <- 0.9 * beta.test
> 
>  mout <- mlogl(beta.test)
> 
>  # omit offset here because it is NULL
>  theta.test <- as.vector(mymodmat %*% beta.test)
> 
>  myval <- sum(ifelse(theta.test < 0,
+      - resp[ , "wins"] * theta.test + rowSums(resp) * log1p(exp(theta.test)),
+      resp[ , "losses"] * theta.test + rowSums(resp) * log1p(exp(- theta.test))))
> 
>  all.equal(mout$value, myval)
[1] TRUE
> 
>  mygrad <- grad(function(beta) mlogl(beta)$value, beta.test)
> 
>  all.equal(mout$gradient, mygrad)
[1] TRUE
> 
>  myhess <- jacobian(function(beta) mlogl(beta)$gradient, beta.test)
> 
>  all.equal(mout$hessian, myhess)
[1] TRUE
>  
>  # now poisson
> 
>  rm(list = ls())
> 
>  data(catrec)
> 
>  gout <- glm(y ~ (.)^3, family = "poisson", data = catrec, x = TRUE)
>  # summary(gout) # do not print summaries in tests, not same on all computers
> 
>  # extract model matrix, response vector, and offset vector
> 
>  modmat <- gout$x
>  mf <- model.frame(gout)
>  resp <- model.response(mf)
>  offs <- model.offset(mf)
> 
>  # modmat # DEBUG only
>  # resp   # DEBUG only
>  # offs   # DEBUG only
> 
>  # have to deal with dropped predictors
>  outies <- is.na(coefficients(gout))
> 
>  mymodmat <- modmat[ , ! outies]
> 
>  mlogl <- glmdr:::make.mlogl(mymodmat, resp, offs, "poisson")
> 
>  beta.test <- coefficients(gout)[! outies]
>  # we don't want to test where the gradient is nearly zero
>  beta.test <- 0.9 * beta.test
> 
>  mout <- mlogl(beta.test)
> 
>  # omit offset here because it is NULL
>  theta.test <- as.vector(mymodmat %*% beta.test)
>  mu.test <- exp(theta.test)
> 
>  myval <- sum(- resp * theta.test + mu.test)
> 
>  all.equal(mout$value, myval)
[1] TRUE
> 
>  mygrad <- grad(function(beta) mlogl(beta)$value, beta.test)
> 
>  all.equal(mout$gradient, mygrad)
[1] TRUE
> 
>  myhess <- jacobian(function(beta) mlogl(beta)$gradient, beta.test)
> 
>  all.equal(mout$hessian, myhess)
[1] TRUE
> 
> 
> proc.time()
   user  system elapsed 
  0.776   0.060   0.831 
